{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsDDevyVuCOz"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -e\n",
        "%%bash\n",
        "set -e\n",
        "\n",
        "cd /content\n",
        "\n",
        "# Download micromamba\n",
        "curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba\n",
        "\n",
        "# Create clean Python 3.11 environment\n",
        "./bin/micromamba create -y -n cb311 -c conda-forge python=3.11 pip\n",
        "\n",
        "echo \"âœ… Environment ready: cb311\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2alxTXqZuGhl"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "cd /content\n",
        "MICROMAMBA=\"/content/bin/micromamba\"\n",
        "\n",
        "echo \"[$(date)] Installing PyTorch 2.5.1 (CUDA 12.1)...\"\n",
        "\"$MICROMAMBA\" run -n cb311 pip install -U pip setuptools wheel\n",
        "\"$MICROMAMBA\" run -n cb311 pip install \\\n",
        "  torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision==0.20.1+cu121 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "echo \"[$(date)] Installing chatterbox-tts (latest from GitHub)...\"\n",
        "\"$MICROMAMBA\" run -n cb311 pip uninstall -y chatterbox-tts || true\n",
        "\"$MICROMAMBA\" run -n cb311 pip install --no-cache-dir --upgrade \\\n",
        "  \"git+https://github.com/resemble-ai/chatterbox.git\"\n",
        "\n",
        "echo \"[$(date)] Installing extra dependencies...\"\n",
        "\"$MICROMAMBA\" run -n cb311 pip install onnx==1.16.0 scipy nltk\n",
        "\n",
        "echo \"âœ… All packages installed!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1slxgHuuLNY"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -e\n",
        "\n",
        "cd /content\n",
        "\n",
        "# Remove old version\n",
        "rm -rf Chatterbox-TTS-Server-main\n",
        "\n",
        "# Clone YOUR forked repo with multilingual support\n",
        "git clone https://github.com/Faheem-saif/Chatterbox-TTS-Server-main.git\n",
        "\n",
        "cd Chatterbox-TTS-Server-main\n",
        "\n",
        "echo \"âœ… Your multilingual server cloned!\"\n",
        "ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8DXZwq1uQEB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import socket\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "PORT = 8004\n",
        "REPO_DIR = \"/content/Chatterbox-TTS-Server-main\"\n",
        "LOG_STDOUT = \"/content/server.log\"\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# Install server dependencies\n",
        "print(\"Installing server requirements...\")\n",
        "subprocess.run([\n",
        "    \"/content/bin/micromamba\", \"run\", \"-n\", \"cb311\",\n",
        "    \"pip\", \"install\", \"-r\", \"requirements-nvidia.txt\"\n",
        "], check=True)\n",
        "\n",
        "# Clear old log\n",
        "Path(LOG_STDOUT).unlink(missing_ok=True)\n",
        "\n",
        "print(f\"\\nStarting multilingual TTS server on port {PORT}...\")\n",
        "print(\"Log file:\", LOG_STDOUT)\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"PYTHONUNBUFFERED\"] = \"1\"\n",
        "env[\"HF_HOME\"] = \"/content/hf_cache\"\n",
        "Path(env[\"HF_HOME\"]).mkdir(exist_ok=True)\n",
        "\n",
        "proc = subprocess.Popen(\n",
        "    [\"/content/bin/micromamba\", \"run\", \"-n\", \"cb311\", \"python\", \"-u\", \"server.py\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        "    bufsize=1,\n",
        "    env=env\n",
        ")\n",
        "\n",
        "# Live log streaming + auto-open UI\n",
        "with open(LOG_STDOUT, \"w\") as f:\n",
        "    shown = False\n",
        "    while True:\n",
        "        line = proc.stdout.readline()\n",
        "        if line:\n",
        "            print(line, end=\"\")\n",
        "            f.write(line)\n",
        "            f.flush()\n",
        "\n",
        "            if not shown and \"Uvicorn running on\" in line:\n",
        "                shown = True\n",
        "                print(\"\\n\" + \"=\"*60)\n",
        "                print(\"ðŸŽ‰ SERVER IS READY! Opening Web UI...\")\n",
        "                print(\"=\"*60)\n",
        "                from google.colab import output\n",
        "                output.serve_kernel_port_as_window(PORT, path=\"/\")\n",
        "\n",
        "        if proc.poll() is not None:\n",
        "            print(\"\\nServer stopped.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvSnFn9AuTVd"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "PORT=8004\n",
        "\n",
        "echo \"Killing processes on port $PORT...\"\n",
        "sudo lsof -t -i:$PORT | xargs -r sudo kill -9 || true\n",
        "\n",
        "echo \"Done. Server stopped.\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
